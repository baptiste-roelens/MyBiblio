{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "18Fpb_yQAyrz8T2Ji73VdkaFmH4aRQD3m",
      "authorship_tag": "ABX9TyPsIiSudSM4csp/e05eXSmT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/baptiste-roelens/MyBiblio/blob/main/20240305_Biblio_with_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "STfZQi_nrxES"
      },
      "outputs": [],
      "source": [
        "#@title Scrape paper informations\n",
        "%%capture\n",
        "!pip -q install pymed==0.8.9\n",
        "!pip -q install paperscraper==0.2.10\n",
        "!pip -q install arxivscraper\n",
        "!pip -q install rich\n",
        "\n",
        "import arxivscraper, textwrap, json, torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import date, datetime, timedelta\n",
        "from paperscraper.get_dumps import biorxiv, medrxiv, chemrxiv\n",
        "from pymed import PubMed\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# date handling\n",
        "def format_date(date, sep):\n",
        "  assert len(sep) == 1\n",
        "  return sep.join([date.strftime(f\"%{x}\") for x in \"Ymd\"])\n",
        "\n",
        "def format_dates(dates, sep):\n",
        "  return [format_date(d, sep) for d in dates]\n",
        "\n",
        "def dates_between(start_date, end_date):\n",
        "  \"\"\"\n",
        "  Returns a list of all dates between two dates (inclusive).\n",
        "  \"\"\"\n",
        "  dates = []\n",
        "  current_date = start_date\n",
        "  while current_date <= end_date:\n",
        "    dates.append(current_date)\n",
        "    current_date += timedelta(days=1)\n",
        "  return dates\n",
        "\n",
        "#get the last date when biblio was assessed\n",
        "with open(\"/content/drive/MyDrive/WIP/Biblio/biblio_date.json\", \"r\") as f:\n",
        "  date_dict = json.load(f)\n",
        "date_start_str = date_dict['year']+ \"/\" + date_dict['month']+ \"/\" + date_dict['day']\n",
        "date_start_datetime = datetime.strptime(date_start_str, \"%Y/%m/%d\")\n",
        "\n",
        "#rXiv dates\n",
        "start_rxivs = format_date(date_start_datetime,  \"-\")\n",
        "end_rxivs = format_date(date.today() - timedelta(days = 1), \"-\")\n",
        "\n",
        "#pubmed dates\n",
        "dates = dates_between(date_start_datetime.date(), date.today() - timedelta(days = 1))\n",
        "pubmed_days = [format_date(d, \"/\") for d in dates]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download paper dumps\n",
        "#Download paper list from rXivs:\n",
        "print(\"medRxiv:\")\n",
        "medrxiv(begin_date=start_rxivs, end_date=end_rxivs, save_path=\"medrxiv.jsonl\")\n",
        "print(\"bioRxiv:\")\n",
        "biorxiv(begin_date=start_rxivs, end_date=end_rxivs, save_path=\"biorxiv.jsonl\")\n",
        "print(\"chemRxiv:\")\n",
        "chemrxiv(begin_date=start_rxivs, end_date=end_rxivs, save_path=\"chemrxiv.jsonl\")\n",
        "\n",
        "#scrape the arxiv q-bio papers\n",
        "print(\"arXiv:\")\n",
        "scraper = arxivscraper.Scraper(category='q-bio', date_from=start_rxivs, date_until=end_rxivs)\n",
        "output = scraper.scrape()\n",
        "cols = ('id', 'title', 'categories', 'abstract', 'doi', 'created', 'updated', 'authors')\n",
        "df_arxiv = pd.DataFrame(output,columns=cols)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "UxjXEaNCz1aZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load classifier and tokenizer\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "id2label = {0: \"Not Relevant\", 1: \"Potentially Interesting\"}\n",
        "label2id = {\"Not Relevant\": 0, \"Potentially Interesting\": 1}\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"batroelens/PubMed_interests\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"batroelens/PubMed_interests\", num_labels=2, id2label=id2label, label2id=label2id).to(\"cuda\")\n",
        "\n",
        "def preprocess_function(abstract):\n",
        "    return tokenizer(abstract, truncation=True, max_length=512, return_tensors='pt').to(\"cuda\")\n",
        "\n",
        "data = {\"Title\": [], \"Abstract\": [], \"Relevance\": [], \"Journal\": [], \"URL\": []}\n",
        "\n",
        "for jsonfile in [\"medrxiv.jsonl\", \"biorxiv.jsonl\", \"chemrxiv.jsonl\"]:\n",
        "  with open(jsonfile) as infile:\n",
        "    for line in infile:\n",
        "      l = json.loads(line)\n",
        "      abstract = l[\"abstract\"].replace(\"\\n\", \" \")\n",
        "      ab = preprocess_function(abstract)\n",
        "      output = model(**ab)\n",
        "      probabilities = torch.softmax(output.logits, dim=1)\n",
        "      class_probabilities = probabilities[0].tolist()\n",
        "      data[\"Title\"].append(l[\"title\"])\n",
        "      data[\"Abstract\"].append(abstract)\n",
        "      data[\"Relevance\"].append(class_probabilities[1])\n",
        "      data[\"Journal\"].append(jsonfile.split(\".\")[0])\n",
        "      data[\"URL\"].append(\"https://doi.org/\"+l[\"doi\"])\n",
        "\n",
        "try:\n",
        "  for pubmed_date in pubmed_days:\n",
        "    pubmed = PubMed(tool=\"MyTool\", email=\"bla@bla.bla\")\n",
        "    search_query = f\"{pubmed_date}[PDAT]\"\n",
        "    results = pubmed.query(search_query, max_results=500)\n",
        "    errors = []\n",
        "    for i, article in enumerate(results):\n",
        "      if article.abstract is None:\n",
        "        continue\n",
        "      ab = preprocess_function(article.abstract)\n",
        "      output = model(**ab)\n",
        "      probabilities = torch.softmax(output.logits, dim=1)\n",
        "      class_probabilities = probabilities[0].tolist()\n",
        "      data[\"Title\"].append(article.title)\n",
        "      data[\"Abstract\"].append(article.abstract.replace(\"\\n\", \" \"))\n",
        "      data[\"Relevance\"].append(class_probabilities[1])\n",
        "      try:\n",
        "        data[\"Journal\"].append(article.journal.strip().replace(\"\\n\", \" \"))\n",
        "      except:\n",
        "        data[\"Journal\"].append(\"NA\")\n",
        "      data[\"URL\"].append(\"https://doi.org/\"+ article.doi.split(\"\\n\")[0])\n",
        "except:\n",
        "  print(\"Pubmed analysis failed\")\n",
        "\n",
        "for index, row in df_arxiv.iterrows():\n",
        "  ab = preprocess_function(df_arxiv.loc[index, 'abstract'])\n",
        "  output = model(**ab)\n",
        "  probabilities = torch.softmax(output.logits, dim=1)\n",
        "  class_probabilities = probabilities[0].tolist()\n",
        "  data[\"Title\"].append(df_arxiv.loc[index, 'title'])\n",
        "  data[\"Abstract\"].append(df_arxiv.loc[index, 'abstract'].replace(\"\\n\", \" \"))\n",
        "  data[\"Relevance\"].append(class_probabilities[1])\n",
        "  data[\"Journal\"].append('arXiv')\n",
        "  data[\"URL\"].append(\"https://doi.org/\"+ df_arxiv.loc[index, 'doi'])"
      ],
      "metadata": {
        "cellView": "form",
        "id": "yOaCLjNX1t3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Display relevant papers\n",
        "from rich.console import Console\n",
        "console = Console()\n",
        "\n",
        "papers = pd.DataFrame.from_dict(data)\n",
        "papers = papers.sort_values(\"Relevance\", ascending=False)\n",
        "selected_papers = papers[papers[\"Relevance\"] >= 0.8]\n",
        "\n",
        "i=1\n",
        "for _, row in selected_papers.iterrows():\n",
        "  prob, title, journal, abstract, url =  row[\"Relevance\"], row[\"Title\"], row[\"Journal\"], row[\"Abstract\"], row[\"URL\"]\n",
        "  console.print(f\"{i}- [bold]{title}[/bold] - [italic]{prob}[/italic] \\n [italic]{journal}[/italic] \\n [cyan]{url}[/cyan]\")\n",
        "  i += 1"
      ],
      "metadata": {
        "cellView": "form",
        "id": "B2FZ8Nmq_l1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Format for adding to TSV for subsequent days.\n",
        "#@markdown Manually set the value at the start of each row to one to indicate a positive example.\n",
        "for _, row in selected_papers.iterrows():\n",
        "  prob, title, journal, abstract =  row[\"Relevance\"], row[\"Title\"], row[\"Journal\"], row[\"Abstract\"]\n",
        "  print(f\"{int(round(prob))} \\t{title} \\t{abstract}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "FHYn78tkvA45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title update stored date in json and disconnect runtime\n",
        "from datetime import date, datetime, timedelta\n",
        "\n",
        "today = date.today()\n",
        "new_date = {'year': str(today.strftime(\"%Y\")), 'month': str(today.strftime(\"%m\")), 'day': str(today.strftime(\"%d\"))}\n",
        "\n",
        "\n",
        "json_date = '/content/drive/MyDrive/WIP/Biblio/biblio_date.json'\n",
        "with open(json_date, \"w\") as f:\n",
        "  json.dump(new_date, f, indent=4)\n",
        "\n",
        "\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "wOfApDJrB4_W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}